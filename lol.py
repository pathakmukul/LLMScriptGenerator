import streamlit as st
import pandas as pd
import openai
import matplotlib.pyplot as plt
# Function to read CSV files and handle encoding issues
def read_csv_file(uploaded_file):
    try:
        return pd.read_csv(uploaded_file)
    except UnicodeDecodeError:
        return pd.read_csv(uploaded_file, encoding='latin1')


# Set up OpenAI API key
openai.api_key = 'sk-'

# Function to generate a description of each DataFrame
def generate_df_description(df, df_name):
    description = f"{df_name}: {df.shape[0]} rows, {df.shape[1]} columns\n"
    for col in df.columns:
        description += f"- {col} (type: {str(df[col].dtype)})\n"
    return description

def generate_df_descriptions(dataframes):
    descriptions = {}
    for df_name, df in dataframes.items():
        description = f"{df_name}: {df.shape[0]} rows, {df.shape[1]} columns\n"
        for col in df.columns:
            description += f"- {col} (type: {str(df[col].dtype)})\n"
        descriptions[df_name] = description
    return descriptions

# Function to query GPT-4 with multiple DataFrame contexts
def query_gpt4(prompt, dataframes):
    df_descriptions = generate_df_descriptions(dataframes)
    combined_description = "\n".join(df_descriptions.values())
    
    full_prompt = f"""You are an expert Python data analyst command builder tool. For the user given task: {prompt}, you will construct a command to perform the user given task. As an output only give the python command, nothing else. DATAFRAMES and their DESCRIPTIONS: \n{combined_description}\nAlways pick column names from the dfs to create queries. For any graph/plot, use streamlit chart elements. For example, to plot a line graph, use st.line_chart(df) and for bar graph, use st.bar_chart(df). For any other graph, use the appropriate streamlit chart elements. 
For st.pyplot() pass an object, to keep it thread-safe. As an output only give the python code, no other text, not even import.
    """

    messages = [{"role": "user", "content": full_prompt}]
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=messages
    )
    return response.choices[0].message['content']

def GPTANALYZER(dataframes, user_prompt, gpt_reply, results):
    df_descriptions = generate_df_descriptions(dataframes)
    combined_description = "\n".join(df_descriptions.values())

    analysis_prompt = f"""Data Analysis Request:
DataFrames Description:{combined_description} User Prompt: {user_prompt} GPT-4 Reply: {gpt_reply} Results: {results}Based on the above information, please provide your analysis and observations as a data analyst.
If the {results} contains "No output generated by the command" or "Error executing command", please only reply: "Try again"
when {results} contain review, Provide a summary of the reviews to act as overall review of the store. Also, justify revenue generated by the store with respect to the review.

"""    

    messages = [{"role": "user", "content": analysis_prompt}]
    response = openai.ChatCompletion.create(
        model="gpt-4",
        messages=messages
    )
    return response.choices[0].message['content']


import matplotlib.pyplot as plt

import io
import contextlib

def execute_command(command, dataframes, st):
    try:
        local_vars = {'st': st, 'plt': plt}
        local_vars.update(dataframes)

        # Adding a line to store the output in a variable 'result'
        command = f'result = {command}'

        compiled_command = compile(command, '<string>', 'exec')
        exec(compiled_command, globals(), local_vars)

        # Handle plotting separately
        if 'plt' in command:
            st.session_state['plot'] = plt.gcf()  # Get the current figure
            return "Plot displayed."

        # Return the captured result
        if 'result' in local_vars:
            return local_vars['result']
        else:
            return "No output generated by the command."
    except Exception as e:
        return f"Error executing command: {e}"




            
# Initialize session state for conversation and DataFrame
if 'conversation' not in st.session_state:
    st.session_state['conversation'] = []
if 'dataframe' not in st.session_state:
    st.session_state['dataframe'] = pd.DataFrame()

# Streamlit UI
st.title('Multi-CSV DataFrame Viewer with GPT-4 Assistance')

# File uploader
uploaded_files = st.file_uploader("Upload CSV Files", type=["csv"], accept_multiple_files=True)

dataframes = {}
if uploaded_files:
    for i, uploaded_file in enumerate(uploaded_files):
        df_name = f"df{i+1}"  # Naming DataFrames as df1, df2, ...
        dataframes[df_name] = read_csv_file(uploaded_file)

# Display DataFrame information in the sidebar
st.sidebar.title("lol5 DataFrame Information")
selected_df_name = st.sidebar.selectbox("Select a DataFrame:", list(dataframes.keys()))
if selected_df_name:
    selected_df = dataframes[selected_df_name]  # Retrieve the selected DataFrame
    df_description = generate_df_description(selected_df, selected_df_name)
    
    st.sidebar.text(df_description)
    st.sidebar.write(f"DataFrame: {selected_df_name}")
    st.sidebar.write("Number of columns:", selected_df.shape[1])
    st.sidebar.write("Number of rows:", selected_df.shape[0])
    st.sidebar.write("Data Types:", selected_df.dtypes)
    st.sidebar.write("---")


# for name, df in dataframes.items():
#     df_description = generate_df_description(df, name)
#     st.sidebar.text(df_description)
#     st.sidebar.write("---")

# Initialize session state for conversation and DataFrame
if 'conversation' not in st.session_state:
    st.session_state['conversation'] = []

# Chat interface and interaction
# Chat interface and interaction
# Chat interface and interaction
user_prompt = st.text_input("Enter your command:", key="promptUser")
if st.button('Send'):
    # Append user input to conversation with styling
    st.session_state.conversation.append(":red[User:] " + user_prompt)

    # Get the command response from GPT-4
    command_response = query_gpt4(user_prompt, dataframes)
    # Append GPT-4's query response to the conversation with styling
    st.session_state.conversation.append(":blue[Query:] " + command_response)
    
    # Execute the command and get the result
    command_result = execute_command(command_response, dataframes, st)
    
    # result_message = ":green[Result:] " + str(command_result)
    # st.session_state.conversation.append(result_message)
    if 'result' in st.session_state:
        st.session_state.conversation.append(":green[Result:] " + str(st.session_state['result']))
        st.write(st.session_state['result'])

    if 'plot' in st.session_state:
        st.session_state.conversation.append(":green[Plot displayed]")
        st.pyplot(st.session_state['plot'])




    # Analyze the conversation and get analysis response
    analysis_response = GPTANALYZER(dataframes, user_prompt, command_response, str(command_result))

    # Append GPT-4's analysis response to the conversation
    st.session_state.conversation.append(":purple[GPT-4 Analysis:] " + analysis_response)

# Display conversation history
with st.container():
    st.markdown("## Conversation History")
    for message in reversed(st.session_state.conversation):
        if message.startswith(":purple[GPT-4 Analysis:]"):
            with st.expander("DatAi Analyst"):
                st.markdown(message, unsafe_allow_html=True)
        else:
            st.markdown(message, unsafe_allow_html=True)


# Use markdown to create a horizontal line for separation
st.markdown("---")

# Display DataFrames
for name, df in dataframes.items():
    with st.container():
        with st.expander(f"Current DataFrame - {name}"):
            st.write(df)
